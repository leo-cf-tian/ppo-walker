{
    "name": "root",
    "gauges": {
        "Walker.Policy.Entropy.mean": {
            "value": 0.27905845642089844,
            "min": 0.27905845642089844,
            "max": 0.5187556147575378,
            "count": 606
        },
        "Walker.Policy.Entropy.sum": {
            "value": 636.2532958984375,
            "min": 572.7061767578125,
            "max": 1593.574462890625,
            "count": 606
        },
        "Walker.Environment.EpisodeLength.mean": {
            "value": 20.05263157894737,
            "min": 19.606896551724137,
            "max": 50.741379310344826,
            "count": 606
        },
        "Walker.Environment.EpisodeLength.sum": {
            "value": 1905.0,
            "min": 932.0,
            "max": 3041.0,
            "count": 606
        },
        "Walker.Step.mean": {
            "value": 6386981.0,
            "min": 4571980.0,
            "max": 6386981.0,
            "count": 606
        },
        "Walker.Step.sum": {
            "value": 6386981.0,
            "min": 4571980.0,
            "max": 6386981.0,
            "count": 606
        },
        "Walker.Policy.ExtrinsicValueEstimate.mean": {
            "value": -389537248.0,
            "min": -394008544.0,
            "max": 1793.1328125,
            "count": 606
        },
        "Walker.Policy.ExtrinsicValueEstimate.sum": {
            "value": -37395574784.0,
            "min": -57131237376.0,
            "max": 130160.1796875,
            "count": 606
        },
        "Walker.Environment.CumulativeReward.mean": {
            "value": -639.5387483295641,
            "min": -654.3504785998114,
            "max": 3077.9138256471547,
            "count": 606
        },
        "Walker.Environment.CumulativeReward.sum": {
            "value": -60756.181091308594,
            "min": -94880.81939697266,
            "max": 206220.22631835938,
            "count": 606
        },
        "Walker.Policy.ExtrinsicReward.mean": {
            "value": -639.5387483295641,
            "min": -654.3504785998114,
            "max": 3077.9138256471547,
            "count": 606
        },
        "Walker.Policy.ExtrinsicReward.sum": {
            "value": -60756.181091308594,
            "min": -94880.81939697266,
            "max": 206220.22631835938,
            "count": 606
        },
        "Walker.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 606
        },
        "Walker.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 606
        },
        "Walker.Losses.PolicyLoss.mean": {
            "value": 0.04423889089375734,
            "min": 0.01236953670110476,
            "max": 0.04423889089375734,
            "count": 88
        },
        "Walker.Losses.PolicyLoss.sum": {
            "value": 0.04423889089375734,
            "min": 0.01236953670110476,
            "max": 0.04423889089375734,
            "count": 88
        },
        "Walker.Losses.ValueLoss.mean": {
            "value": 1.1557354040849614e+20,
            "min": 73140.26354166666,
            "max": 1.1557354040849614e+20,
            "count": 88
        },
        "Walker.Losses.ValueLoss.sum": {
            "value": 1.1557354040849614e+20,
            "min": 73140.26354166666,
            "max": 1.1557354040849614e+20,
            "count": 88
        },
        "Walker.Policy.LearningRate.mean": {
            "value": 0.0009787474221252578,
            "min": 0.0009787474221252578,
            "max": 0.0009846948281971844,
            "count": 88
        },
        "Walker.Policy.LearningRate.sum": {
            "value": 0.0009787474221252578,
            "min": 0.0009787474221252578,
            "max": 0.0009846948281971844,
            "count": 88
        },
        "Walker.Policy.Epsilon.mean": {
            "value": 0.19787474199999996,
            "min": 0.19787474199999996,
            "max": 0.19846948266666672,
            "count": 88
        },
        "Walker.Policy.Epsilon.sum": {
            "value": 0.19787474199999996,
            "min": 0.19787474199999996,
            "max": 0.19846948266666672,
            "count": 88
        },
        "Walker.Policy.Beta.mean": {
            "value": 0.004893949625800001,
            "min": 0.004893949625800001,
            "max": 0.004923627185066668,
            "count": 88
        },
        "Walker.Policy.Beta.sum": {
            "value": 0.004893949625800001,
            "min": 0.004893949625800001,
            "max": 0.004923627185066668,
            "count": 88
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1713592002",
        "python_version": "3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:01:18) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\leocf\\.conda\\envs\\mlagents\\Scripts\\mlagents-learn C:\\Users\\leocf\\OneDrive\\Documents\\My Programs\\ML Playground\\Assets\\Models\\results\\ppo\\configuration.yaml --resume",
        "mlagents_version": "1.0.0",
        "mlagents_envs_version": "1.0.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.2+cu121",
        "numpy_version": "1.23.2",
        "end_time_seconds": "1713595536"
    },
    "total": 3533.587393499998,
    "count": 1,
    "self": 0.005728799995267764,
    "children": {
        "run_training.setup": {
            "total": 0.06622730000526644,
            "count": 1,
            "self": 0.06622730000526644
        },
        "TrainerController.start_learning": {
            "total": 3533.5154373999976,
            "count": 1,
            "self": 4.039615600806428,
            "children": {
                "TrainerController._reset_env": {
                    "total": 8.077499000006355,
                    "count": 1,
                    "self": 8.077499000006355
                },
                "TrainerController.advance": {
                    "total": 3521.299395899172,
                    "count": 261065,
                    "self": 3.779592594568385,
                    "children": {
                        "env_step": {
                            "total": 2887.9120494018425,
                            "count": 261065,
                            "self": 2319.926832590776,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 565.4109861145553,
                                    "count": 261067,
                                    "self": 11.551846013782779,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 553.8591401007725,
                                            "count": 227432,
                                            "self": 553.8591401007725
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.574230696511222,
                                    "count": 261064,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 3456.8617132997897,
                                            "count": 261064,
                                            "is_parallel": true,
                                            "self": 1430.766040210845,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0015934999682940543,
                                                    "count": 3,
                                                    "is_parallel": true,
                                                    "self": 0.000311399984639138,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0012820999836549163,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.0012820999836549163
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 2026.0940795889765,
                                                    "count": 261064,
                                                    "is_parallel": true,
                                                    "self": 43.668609082407784,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 58.652986907807644,
                                                            "count": 261064,
                                                            "is_parallel": true,
                                                            "self": 58.652986907807644
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1801.1617384009878,
                                                            "count": 261064,
                                                            "is_parallel": true,
                                                            "self": 1801.1617384009878
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 122.61074519777321,
                                                            "count": 261064,
                                                            "is_parallel": true,
                                                            "self": 23.92799380008364,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 98.68275139768957,
                                                                    "count": 1044256,
                                                                    "is_parallel": true,
                                                                    "self": 98.68275139768957
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 629.607753902761,
                            "count": 261064,
                            "self": 5.088952412159415,
                            "children": {
                                "process_trajectory": {
                                    "total": 310.7629239906382,
                                    "count": 261064,
                                    "self": 310.47824219064205,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.2846817999961786,
                                            "count": 3,
                                            "self": 0.2846817999961786
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 313.7558774999634,
                                    "count": 88,
                                    "self": 223.11229929944966,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 90.64357820051373,
                                            "count": 2640,
                                            "self": 90.64357820051373
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.200009137392044e-06,
                    "count": 1,
                    "self": 1.200009137392044e-06
                },
                "TrainerController._save_models": {
                    "total": 0.0989257000037469,
                    "count": 1,
                    "self": 0.00755630002822727,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.09136939997551963,
                            "count": 1,
                            "self": 0.09136939997551963
                        }
                    }
                }
            }
        }
    }
}